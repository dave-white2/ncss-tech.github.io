---
title: "Accuracy and Uncertainty"
date: "`r Sys.Date()`"
author: D.E. Beaudette
output:
  html_document:
    fig_caption: yes
    number_sections: no
    theme: journal
    smart: no
bibliography: bibliography.bib
---
  
```{r setup, echo=FALSE, results='hide', warning=FALSE}
library(knitr, quietly=TRUE)
opts_chunk$set(message=FALSE, warning=FALSE, background='#F7F7F7', fig.retina=2, dev='png', tidy=FALSE, verbose=FALSE, fig.align='center')
options(width=100, stringsAsFactors=FALSE)
```

# Background

This docment is my attempt at addressing some of the issues related to accuracy and uncertainty that I have brought up on the phone conferences. As such, it is very much a **work in progress**.

Our current QC/QA process is based on many forms of evaluation, accumulates some degree of subjectivity and relies heavily on qualitative forms of information (field experience, institutional knowledge, etc.).  On the opposite side of the spectrum, the validation of raster mapping is often claimed to be free of subjective interference and entirely quantitative. Those are "good things" that we should always strive for, however, the simplicity of calculating a "percent correctly classified" can interfere with a more nuanced evaluation of accuracy. As I mentioned on the phone (and implicitly volunteered for) a validation "score" might be more meaningful than any single validation metrics. One such score might include:

  * agreement between predicted probabilities and observed class (e.g. Brier scores)
  * agreement between most-likely class and observed class, accounting for class similarities
  * distribution of class-wise Shannon entropy values
  * prior vs. predicted vs. validation proportion of classes
  * some kind of metric that integrates spatial connectivity of predictions / observations, for example: cross-tabulate predictions / validation by geomorphon classes

I strongly believe that we need a robust suite of metrics primarily for internal discussion and evaluation of raster mapping products; even more so when complex modeling frameworks such as randomforest or neural nets are used.

## Accuracy

Confusion matrix

### Brier Scores
https://en.wikipedia.org/wiki/Brier_score

Brier scores [@Harrell2001] quantify agreement between observed classes and predicted probabilities:
$$ B = \frac{1}{n} \sum_{i=1}^{n}{ ( p_{i} - y_{i} )^{2}  }  $$
where $B$ is an index of agreement between predicted probabilities, $\mathbf{p}$, and class labels, $\mathbf{y}$. Larger values suggest less agreement between probabilities and observed class labels.

What about a weighted version of this score, based on a re-statement of the distance matrix?

### Tau with Class Distances (similarity-weighted)

[@Rossiter2017] implemented in `aqp::tauw()`. This paper contains some discussion on a weighted version of Shannon Entropy... (?)

```{r eval=FALSE, echo=FALSE}
## from tauw manual page.

# example confusion matrix
# rows: allocation (user's counts)
# columns: reference (producer's counts)
crossclass <- matrix(data=c(2,1,0,5,0,0,
                            1,74,2,1,3,6,
                            0,5,8,6,1,3,
                            6,1,3,91,0,0,
                            0,4,0,0,0,4,
                            0,6,2,2,4,38),
                     nrow=6, byrow=TRUE)
row.names(crossclass) <- c("OP", "SA", "UA", "UC", "AV", "AC")
colnames(crossclass) <- row.names(crossclass)

# build the weights matrix
# how much credit for a mis-allocation
weights <- matrix(data=c(1.00,0.05,0.05,0.15,0.05,0.15,
                         0.05,1.00,0.05,0.05,0.05,0.35,
                         0.05,0.05,1.00,0.20,0.15,0.15,
                         0.15,0.05,0.25,1.00,0.10,0.25,
                         0.05,0.10,0.15,0.10,1.00,0.15,
                         0.20,0.30,0.10,0.25,0.20,1.00),
                  nrow=6, byrow=TRUE)

# unweighted accuracy
summaryTauW(nnaive <- tauW(crossclass))

# unweighted tau with equal priors, equivalent to Foody (1992) modified Kappa
tauW(crossclass)$tau

# unweighted tau with user's = producer's marginals, equivalent to original kappa
(priors <-  apply(crossclass, 2, sum)/sum(crossclass))
tauW(crossclass, P=priors)$tau

# weighted accuracy; tau with equal priors
summaryTauW(weighted <- tauW(crossclass, W=weights))

# weighted accuracy; tau with user's = producer's marginals
summaryTauW(tauW(crossclass, W=weights, P=priors))

```

### Other Methods
  * purity
  * class representation
  * raw confusion matrix
  * ???


## Uncertainty

## Shannon Entropy

A normalized version [@Kempen2009] of Shannon entropy can be calculated:

$$ H = -\sum_{i=1}^{n}{p_{i} * log_{n}(p_{i})}  $$
where $H$ is an index of uncertainty associated with predicted probabilities, $\mathbf{p}$, of encountering classes $i$ through $n$. Values range from 0 (maximum information, minimum entropy) to 1 (minumum information, maximum entropy).

### Weighted Shannon Entropy

A simple extension (weighted entropy score) of the standard version is described in [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4029130/):

$$ \mathit{WES} = -\sum_{i=1}^{n}{w_{i} * p_{i} * log_{n}(p_{i})}  $$
where $w_{i}$ is the weight associated with $p_{i]}$. This isn't quite applicable as the weights (we are interested in) are based on pair-wise distances between classes. Perhaps the theory of [mutual information](https://en.wikipedia.org/wiki/Mutual_information) or [joint entropy](https://en.wikipedia.org/wiki/Joint_entropy) are the more appropriate route.

## Confusion Index

Following [@Burrough1997]:

$$ \mathit{CI} = [1 - (\mu_{max} - \mu_{max-1})] $$
where $\mathit{CI}$ is an index of confusion between the first most likely, $\mu_{max}$, and second most likey, $\mu_{max-1}$, class probabilities.


# Examples via Simulation
**this isn't done yet**

Drawing from Dirichlet distribution:

  * interpretation of alpha isn't that simple:
    + http://projects.csail.mit.edu/church/w/images/thumb/7/73/Dirichlet.png/400px-Dirichlet.png
    + https://stats.stackexchange.com/questions/244917/what-exactly-is-the-alpha-in-the-dirichlet-distribution/244946#244946
    + https://www.hakkalabs.co/articles/the-dirichlet-distribution
    + http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/
  * single simulation fixed alpha
  * many simulations with several cases:
    + high uncertainty
    + low uncertainty
    + complex mixture


## Shannon entropy vs. confusion index
```{r}
library(igraph)
library(latticeExtra)
library(plyr)
library(reshape2)

# normalized Shannon entropy (Kempen et al, 2009)
shannon.H <- function(i, b) {
  res <- -1 * sum(i * log(i, base=b))
  return(res)
}

# confusion index (Burrough et al., 1997)
confusion.index <- function(i) {
  i <- sort(i, decreasing = TRUE)
  res <- 1 - (i[1] - i[2])
  return(res)
}


# simulate predicted class probabilities via draws from dirichlet 
# (n pixel * k classes)
# n: number of simulated "pixels" 
# k: number of classes
# alpha: dirichlet hyperparameter (class likelyhood)
simulate.predictions <- function(n, k, alpha) {
  # generate simulated 
  x <- sample_dirichlet(n, alpha = alpha)
  x <- t(x)
  
  # reshape
  d <- as.data.frame(x)
  d$id <- rownames(d)
  m <- melt(d, id.vars = 'id')
  
  # compute H and CI for each simulated case
  H <- apply(x, 1, shannon.H, b=k)
  CI <- apply(x, 1, confusion.index)
  
  # reshape for plotting
  z <- data.frame(H=H, CI=CI)
  z.long <- make.groups(H=z$H, CI=z$CI)
  
  return(list(predictions=m, stats=z, stats.long=z.long))
}
```


```{r, fig.width=10, fig.height=4}
s <- list()

alpha <- c(1,10,2,20,25,3,5,6,3,2)
s[['Case 1']] <- simulate.predictions(n=1000, k=10, alpha)

alpha <- c(1,1,2,3,3,3,5,6,3,25)
s[['Case 2']] <- simulate.predictions(n=1000, k=10, alpha)

alpha <- c(1,1,1,1,1,1,1,1,1,50)
s[['Case 3']] <- simulate.predictions(n=1000, k=10, alpha)

# stack class probs
ss <- ldply(s, function(i) i$predictions)

# plot class probabilities
p.1 <- densityplot( ~ value | .id, groups=variable, data=ss, 
pch=NA, xlim=c(-0.1, 1.1), scales=list(alternating=3, x=list(tick.number=5)), xlab='Class Probability',
strip=strip.custom(bg=grey(0.85)))

print(p.1)
```


```{r, fig.width=10, fig.height=4}
# stack stats
cp <- ldply(s, function(i) i$stats.long)

p.2 <- densityplot( ~ data | .id, groups=which, data=cp, pch=NA, auto.key=list(columns=2, lines=TRUE, points=FALSE), xlim=c(-0.1, 1.1), strip=strip.custom(bg=grey(0.85)), scales=list(alternating=3, y=list(rot=0), x=list(tick.number=5)), xlab='')

print(p.2)
```

```{r fig.width=6, fig.height=6}
ss <- ldply(s, function(i) i$stats)

# how do these two compare in general?
xyplot(CI ~ H, groups=.id, data=ss, asp=1, auto.key = list(columns=3, lines=FALSE, points=TRUE), par.settings=list(superpose.symbol=list(alpha=0.5)))

ddply(ss, '.id', .fun=summarize, cor=cor(CI, H, method = 'spearman'))
```



# Real Examples


# Resources

 * http://biostat.mc.vanderbilt.edu/wiki/Main/RmS

# References
fake text here








