---
title: "Accuracy and Uncertainty"
date: "`r Sys.Date()`"
author: D.E. Beaudette
output:
  html_document:
    fig_caption: yes
    number_sections: no
    theme: journal
    smart: no
bibliography: bibliography.bib
---
  
```{r setup, echo=FALSE, results='hide', warning=FALSE}
library(knitr, quietly=TRUE)
library(kableExtra, quietly=TRUE)
opts_chunk$set(message=FALSE, warning=FALSE, background='#F7F7F7', fig.retina=2, dev='png', tidy=FALSE, verbose=FALSE, fig.align='center', echo=FALSE)
options(width=100, stringsAsFactors=FALSE)
```

```{r data-prep-and-function}
library(igraph)
library(latticeExtra)
library(plyr)
library(reshape2)
library(wesanderson)
library(grid)

# normalized Shannon entropy (Kempen et al, 2009)
shannon.H <- function(i, b) {
  res <- -1 * sum(i * log(i, base=b))
  return(res)
}

# confusion index (Burrough et al., 1997)
confusion.index <- function(i) {
  i <- sort(i, decreasing = TRUE)
  res <- 1 - (i[1] - i[2])
  return(res)
}


# simulate predicted class probabilities via draws from dirichlet 
# (n pixel * k classes)
# n: number of simulated "pixels" 
# alpha: dirichlet hyperparameter (class likelihood)
simulate.predictions <- function(n, alpha) {
  # number of classes
  k <- length(alpha)
  
  # generate simulated probabilities
  x <- sample_dirichlet(n, alpha = alpha)
  x <- t(x)
  
  # reshape
  d <- as.data.frame(x)
  d$id <- rownames(d)
  m <- melt(d, id.vars = 'id')
  m$variable <- factor(m$variable, labels = paste0(c('soil '), toupper(letters[1:k])))
  
  # compute H and CI for each simulated case
  H <- apply(x, 1, shannon.H, b=k)
  CI <- apply(x, 1, confusion.index)
  
  # reshape for plotting
  z <- data.frame(H=H, CI=CI)
  z.long <- make.groups(H=z$H, CI=z$CI)
  
  return(list(predictions=m, stats=z, stats.long=z.long))
}
```

```{r simulate-date}
s <- list()
n <- 1000

alpha.1 <- c(4,10,5,22,25)
s[['Case 1']] <- simulate.predictions(n=n, alpha=alpha.1)

alpha.2 <- c(1,3,5,6,15)
s[['Case 2']] <- simulate.predictions(n=n, alpha=alpha.2)

alpha.3 <- c(2,2,2,2,40)
s[['Case 3']] <- simulate.predictions(n=n, alpha=alpha.3)

# stack class probs
ss <- ldply(s, function(i) i$predictions)
names(ss)[1] <- 'example'
ss$example <- factor(ss$example)
```

```{r make-figures}
## class probabilities
cols <- brewer.pal(9, 'Set1')
tps <- list(superpose.line=list(col=cols, lwd=1, alpha=0.85))

p.1 <- densityplot( ~ value | example, groups=variable, data=ss, 
pch=NA, xlim=c(-0.1, 1.1), scales=list(alternating=3, x=list(tick.number=5)), xlab='Class Probability',
strip=strip.custom(bg=grey(0.85)), auto.key=list(columns=5, lines=TRUE, points=FALSE),
par.settings=tps, panel=function(...) {
  gs <- seq(0,1, by=0.1)
  panel.abline(v=gs, lty=3, col='grey')
  panel.densityplot(...)
})


## stack stats
ss <- ldply(s, function(i) i$stats.long)
names(ss)[1] <- 'example'
ss$example <- factor(ss$example)

# uncertainty metrics
cols <- wes_palette("Zissou")[c(1,5)]
tps <- list(superpose.line=list(col=cols, lwd=2, alpha=0.85))

p.2 <- densityplot( ~ data | example, groups=which, data=ss, pch=NA, auto.key=list(columns=2, lines=TRUE, points=FALSE), xlim=c(-0.1, 1.1), strip=strip.custom(bg=grey(0.85)), scales=list(alternating=3, y=list(rot=0), x=list(tick.number=5)), xlab='', par.settings=tps, panel=function(...) {
  gs <- seq(0,1, by=0.1)
  panel.abline(v=gs, lty=3, col='grey')
  panel.densityplot(...)
})


## scatter plot of H vs. CI
ss <- ldply(s, function(i) i$stats)
names(ss)[1] <- 'example'
ss$example <- factor(ss$example)

# how do these two compare in general?
p.3 <- xyplot(H ~ CI | example, data=ss, asp=1, auto.key = list(columns=3, lines=FALSE, points=TRUE),
scales=list(alternating=1, at=seq(0, 1, by=0.2)), xlim=c(-0.1, 1.1), ylim=c(-0.1, 1.1), xlab='Confusion Index', ylab='Shannon H',
strip=strip.custom(bg=grey(0.85)), par.settings=list(plot.symbol=list(col='royalblue', pch=16, cex=0.85, alpha=0.25)), 
panel=function(x, y, subscripts=subscripts, ...) {
  gs <- seq(0,1, by=0.1)
  l <- lm(y ~ x)
  med.H <- round(median(y), 2)
  med.CI <- round(median(x), 2)
  iqr.H <- round(IQR(y), 2)
  iqr.CI <- round(IQR(x), 2)
  cor.xy <- round(cor(x, y, method = 'spearman'), 2)
  ann <- paste0('H: ', med.H, ' (', iqr.H, ')\n', 
                'CI: ', med.CI, ' (', iqr.CI, ')\n',
                'cor: ', cor.xy)

  panel.abline(h=gs, v=gs, lty=3, col='grey')
  panel.points(ss$CI, ss$H, col=grey(0.85), alpha=0.1)
  panel.xyplot(x, y, subscripts=subscripts, ...)
  panel.abline(l, col='black', lwd=1.5, ...)
  panel.abline(0, 1, lty=2)
  grid.text(ann, x = unit(0.05, 'npc'), unit(0.95, 'npc'), just = c('left', 'top'), gp = gpar(cex=0.75, font=2))
})

stats.cor <- ddply(ss, 'example', .fun=plyr::summarize, 
                   H.med=median(H), CI.med=median(CI),
                   H.iqr=IQR(H), CI.iqr=IQR(CI),
                   cor=cor(CI, H, method = 'spearman')
                   )

```



# Background

This docment is my attempt at addressing some of the issues related to accuracy and uncertainty that I have brought up on the phone conferences. As such, it is very much a **work in progress**.

Our current QC/QA process is based on many forms of evaluation, accumulates some degree of subjectivity and relies heavily on qualitative forms of information (field experience, institutional knowledge, etc.).  On the opposite side of the spectrum, the validation of raster mapping is often claimed to be free of subjective interference and entirely quantitative. Those are "good things" that we should always strive for, however, the simplicity of calculating a "percent correctly classified" can interfere with a more nuanced evaluation of accuracy. As I mentioned on the phone (and implicitly volunteered for) a validation "score" might be more meaningful than any single validation metrics. One such score might include:

  * agreement between predicted probabilities and observed class (e.g. Brier scores)
  * agreement between most-likely class and observed class, accounting for class similarities
  * distribution of class-wise Shannon entropy values
  * prior vs. predicted vs. validation proportion of classes
  * some kind of metric that integrates spatial connectivity of predictions / observations, for example: cross-tabulate predictions / validation by geomorphon classes

I strongly believe that we need a robust suite of metrics primarily for internal discussion and evaluation of raster mapping products; even more so when complex modeling frameworks such as randomforest or neural nets are used.

## Accuracy

Confusion matrix

### Brier Scores
https://en.wikipedia.org/wiki/Brier_score

Brier scores [@Harrell2001] quantify agreement between observed classes and predicted probabilities:
$$ B = \frac{1}{n} \sum_{i=1}^{n}{ ( p_{i} - y_{i} )^{2}  }  $$
where $B$ is an index of agreement between predicted probabilities, $\mathbf{p}$, and class labels, $\mathbf{y}$. Larger values suggest less agreement between probabilities and observed class labels.

**TODO**
  1. What about a weighted version of this score, based on a re-statement of the distance matrix?
  2. Add general purpose `brierScore()` function to aqp, adapt from `get.ml.hz()`.

### Tau with Class Distances (similarity-weighted)

[@Rossiter2017] implemented in `aqp::tauw()`. This paper contains some discussion on a weighted version of Shannon Entropy... (?)

```{r eval=FALSE}
## from tauw manual page.

# example confusion matrix
# rows: allocation (user's counts)
# columns: reference (producer's counts)
crossclass <- matrix(data=c(2,1,0,5,0,0,
                            1,74,2,1,3,6,
                            0,5,8,6,1,3,
                            6,1,3,91,0,0,
                            0,4,0,0,0,4,
                            0,6,2,2,4,38),
                     nrow=6, byrow=TRUE)
row.names(crossclass) <- c("OP", "SA", "UA", "UC", "AV", "AC")
colnames(crossclass) <- row.names(crossclass)

# build the weights matrix
# how much credit for a mis-allocation
weights <- matrix(data=c(1.00,0.05,0.05,0.15,0.05,0.15,
                         0.05,1.00,0.05,0.05,0.05,0.35,
                         0.05,0.05,1.00,0.20,0.15,0.15,
                         0.15,0.05,0.25,1.00,0.10,0.25,
                         0.05,0.10,0.15,0.10,1.00,0.15,
                         0.20,0.30,0.10,0.25,0.20,1.00),
                  nrow=6, byrow=TRUE)

# unweighted accuracy
summaryTauW(nnaive <- tauW(crossclass))

# unweighted tau with equal priors, equivalent to Foody (1992) modified Kappa
tauW(crossclass)$tau

# unweighted tau with user's = producer's marginals, equivalent to original kappa
(priors <-  apply(crossclass, 2, sum)/sum(crossclass))
tauW(crossclass, P=priors)$tau

# weighted accuracy; tau with equal priors
summaryTauW(weighted <- tauW(crossclass, W=weights))

# weighted accuracy; tau with user's = producer's marginals
summaryTauW(tauW(crossclass, W=weights, P=priors))

```

### Other Methods
  * purity
  * class representation
  * raw confusion matrix
  * ???


## Uncertainty

## Shannon Entropy

A normalized version [@Kempen2009] of Shannon entropy can be calculated:

$$ H = -\sum_{i=1}^{n}{p_{i} * log_{n}(p_{i})}  $$
where $H$ is an index of uncertainty associated with predicted probabilities, $\mathbf{p}$, of encountering classes $i$ through $n$. Values range from 0 (maximum information, minimum entropy) to 1 (minumum information, maximum entropy).

### Weighted Shannon Entropy

A simple extension (weighted entropy score) of the standard version is described in [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4029130/):

$$ \mathit{WES} = -\sum_{i=1}^{n}{w_{i} * p_{i} * log_{n}(p_{i})}  $$
where $w_{i}$ is the weight associated with $p_{i]}$. This isn't quite applicable as the weights (we are interested in) are based on pair-wise distances between classes. Perhaps the theory of [mutual information](https://en.wikipedia.org/wiki/Mutual_information) or [joint entropy](https://en.wikipedia.org/wiki/Joint_entropy) are the more appropriate route.

## Confusion Index

Following [@Burrough1997]:

$$ \mathit{CI} = [1 - (\mu_{max} - \mu_{max-1})] $$
where $\mathit{CI}$ is an index of confusion between the first most likely, $\mu_{max}$, and second most likey, $\mu_{max-1}$, class probabilities.


# Examples via Simulation
**this isn't done yet**

Drawing from Dirichlet distribution:

  * interpretation of alpha isn't that simple:
    + http://projects.csail.mit.edu/church/w/images/thumb/7/73/Dirichlet.png/400px-Dirichlet.png
    + https://stats.stackexchange.com/questions/244917/what-exactly-is-the-alpha-in-the-dirichlet-distribution/244946#244946
    + https://www.hakkalabs.co/articles/the-dirichlet-distribution
    + http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/
  * single simulation fixed alpha
  * many simulations with several cases:
    + high uncertainty
    + low uncertainty
    + complex mixture


## Shannon entropy vs. confusion index
Consider a probabalistic soil class model, that generates predictions for 5 possible classes. Suites of predicted probabilities fall into 3 general cases:

```{r}
extractExample <- function(x, n=1) {
  p <- dcast(x$predictions, id ~ variable, value.var = 'value')[1:n, ]
  stats <- x$stats[1:n, ]
  d <- data.frame(p[, -1], stats)
  return(d)
}

ex <- ldply(s, extractExample, n=1)
names(ex)[1] <- 'example'

kableExtra::kable_styling(knitr::kable(ex, row.names = FALSE, digits = 2, format='html'), full_width = FALSE)
```


```{r, fig.width=10, fig.height=4, fig.cap='Probability distributions of model predictions, 1000 simulations.'}
# examples of three cases
print(p.1)
```


```{r, fig.width=10, fig.height=4, fig.cap='Distribution of H and CI values computed from simulated predictions.'}
# uncertainty metrics
print(p.2)
```

```{r fig.width=10, fig.height=5, fig.cap='Relationship between H and CI values. Annotations are medians (IQR) and Spearman rank correlation.'}
print(p.3)

# kableExtra::kable_styling(knitr::kable(stats.cor, row.names = FALSE, digits = 2, format='html'), full_width = FALSE)
```



# Real Examples


# Resources

 * http://biostat.mc.vanderbilt.edu/wiki/Main/RmS

# References
fake text here








