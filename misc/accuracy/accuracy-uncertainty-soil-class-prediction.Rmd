---
title: "Accuracy and Uncertainty"
date: "`r Sys.Date()`"
author: D.E. Beaudette
output:
  html_document:
    fig_caption: yes
    number_sections: no
    theme: journal
    smart: no
bibliography: bibliography.bib
---
  
```{r setup, echo=FALSE, results='hide', warning=FALSE}
library(knitr, quietly=TRUE)
library(kableExtra, quietly=TRUE)
opts_chunk$set(message=FALSE, warning=FALSE, background='#F7F7F7', fig.retina=2, dev='png', tidy=FALSE, verbose=FALSE, fig.align='center', echo=FALSE)
options(width=100, stringsAsFactors=FALSE)
```

```{r data-prep-and-function}
library(igraph)
library(latticeExtra)
library(plyr)
library(reshape2)
library(wesanderson)
library(grid)
library(aqp)

## TODO: this stuff should be integrate into one of the AQP suite
source('local-functions.R')

```

```{r simulate-date}
s <- list()
n <- 1000

alpha.1 <- c(4,10,5,22,25)
s[['Case 1']] <- simulatePredictions(n=n, alpha=alpha.1)

alpha.2 <- c(3,4,5,6,15)
s[['Case 2']] <- simulatePredictions(n=n, alpha=alpha.2)

alpha.3 <- c(2,2,2,2,40)
s[['Case 3']] <- simulatePredictions(n=n, alpha=alpha.3)
```

```{r make-figures}
## class probabilities
ss <- ldply(s, function(i) i$predictions.long)
names(ss)[1] <- 'example'
ss$example <- factor(ss$example)

cols <- brewer.pal(9, 'Set1')
tps <- list(superpose.line=list(col=cols, lwd=1, alpha=0.85))

p.1 <- densityplot( ~ value | example, groups=variable, data=ss, as.table=TRUE, layout=c(3,1),
pch=NA, xlim=c(-0.1, 1.1), scales=list(alternating=3, x=list(tick.number=5)), xlab='Class Probability',
strip=strip.custom(bg=grey(0.85)), auto.key=list(columns=5, lines=TRUE, points=FALSE),
par.settings=tps, panel=function(...) {
  gs <- seq(0,1, by=0.1)
  panel.abline(v=gs, lty=3, col='grey')
  panel.densityplot(...)
})


## distribution of stats
ss <- ldply(s, function(i) i$stats.long)
names(ss)[1] <- 'example'
ss$example <- factor(ss$example)

# uncertainty metrics
cols <- wes_palette("Zissou")[c(1,5)]
tps <- list(superpose.line=list(col=cols, lwd=2, alpha=0.85))

p.2 <- densityplot( ~ data | example, groups=which, data=ss, as.table=TRUE, layout=c(3,1), pch=NA, auto.key=list(columns=2, lines=TRUE, points=FALSE), strip=strip.custom(bg=grey(0.85)), scales=list(alternating=3, y=list(rot=0), x=list(tick.number=5)), xlab='', par.settings=tps, panel=function(...) {
  gs <- seq(0, 2.5, by=0.25)
  panel.abline(v=gs, lty=3, col='grey')
  panel.densityplot(...)
})


## scatter plot of H vs. CI
ss <- ldply(s, function(i) i$stats)
names(ss)[1] <- 'example'
ss$example <- factor(ss$example)

# xlim=c(-0.1, 1.1), ylim=c(-0.1, 1.1),

# how do these two compare in general?
p.3 <- xyplot(Shannon.H ~ CI | example, data=ss, auto.key = list(columns=3, lines=FALSE, points=TRUE), as.table=TRUE, layout=c(3,1),
scales=list(alternating=1),  xlab='Confusion Index', ylab='Shannon H',
strip=strip.custom(bg=grey(0.85)), par.settings=list(plot.symbol=list(col='royalblue', pch=16, cex=0.85, alpha=0.25)), 
panel=function(x, y, subscripts=subscripts, ...) {
  panel.grid(-1, -1, lty=3, col='grey')
  l <- lm(y ~ x)
  med.H <- round(median(y), 2)
  med.CI <- round(median(x), 2)
  iqr.H <- round(IQR(y), 2)
  iqr.CI <- round(IQR(x), 2)
  cor.xy <- round(cor(x, y, method = 'spearman'), 2)
  ann <- paste0('H: ', med.H, ' (', iqr.H, ')\n', 
                'CI: ', med.CI, ' (', iqr.CI, ')\n',
                'cor: ', cor.xy)
  
  panel.points(ss$CI, ss$Shannon.H, col=grey(0.85), alpha=0.1)
  panel.xyplot(x, y, subscripts=subscripts, ...)
  panel.abline(l, col='black', lwd=1.5, ...)
  panel.abline(0, 1, lty=2)
  grid.text(ann, x = unit(0.05, 'npc'), unit(0.95, 'npc'), just = c('left', 'top'), gp = gpar(cex=0.75, font=2))
})

stats.cor <- ddply(ss, 'example', .fun=plyr::summarize, 
                   H.med=median(Shannon.H), CI.med=median(CI),
                   H.iqr=IQR(Shannon.H), CI.iqr=IQR(CI),
                   cor=cor(CI, Shannon.H, method = 'spearman')
                   )

```

```{r eval=FALSE}
# H as a function of n
plot(1:1000, sapply(1:1000, function(i) shannon.H(rep(1/i, times=i))), type='l')

```


# Background

This document is an attempt at addressing some of the issues related to accuracy and uncertainty that I have brought up on the phone conferences.

Our current QC/QA process is based on many forms of evaluation, accumulates some degree of subjectivity and relies heavily on qualitative forms of information (field experience, institutional knowledge, etc.).  On the opposite side of the spectrum, the validation of raster mapping is often claimed to be free of subjective interference and entirely quantitative. Those are "good things" that we should always strive for, however, the simplicity of calculating a "percent correctly classified" can interfere with a more nuanced evaluation of accuracy. As I mentioned on the phone (and implicitly volunteered for) a validation "score" might be more meaningful than any single validation metrics. One such score might include:

  * agreement between predicted probabilities and observed class (e.g. Brier scores)
  * agreement between the most likely class and observed class, accounting for class similarities (e.g. weighted Tau)
  * distribution of class-wise Shannon entropy values
  * calibration vs. predicted vs. validation proportion of classes
  * some kind of metric that integrates spatial connectivity of predictions / observations, for example: cross-tabulate calibration / prediction / validation classes with geomorphon classes

I strongly believe that we need a robust suite of metrics primarily for internal discussion and evaluation of raster mapping products; even more so when complex modeling frameworks such as randomForest or neural nets are used.


# Concept Demonstration via Simulated Data

## Simulation via Dirichlet

Consider a supervised classification that generates predictions for 5 possible soil classes. Suites of predicted probabilities fall into 3 general cases:

   * "Case 1": classes **D** and **E** are nearly tied for the most likely class, but their respective probabilities are generally < 0.5
   * "Case 2": class **E** is almost always the most likely class, but classes **B**, **C**, and **D** are tied for second place
   * "Case 3": class **E** is always the most likely class, all other classes have probabilities < 0.2

```{r, fig.width=10, fig.height=4, fig.cap='Probability distributions of predictions.'}
# examples of three cases
print(p.1)
```

Even though these are simulated data, the three cases above demonstrate common modeling scenarios where classification uncertainty ranges from very low ("Case 3") in some areas to quite high ("Case 1") in others. These three cases could easily be associated with real situations:

  * "Case 1": predictions for soil classes represent a hillslope complex that isn't quite disentangled by the model
  * "Case 2": predictions for soil classes represent limited success in partitioning between a single water shedding (**E**) vs. multiple water collecting positions (**A**-**D**)
  * "Case 3": predictions for soil classes represent a successful partitioning between Holocene age deposits (**E**) vs. older alluvial terraces (**A**-**D**)




### Follow-Up / Sanity-Check
Drawing from Dirichlet distribution:

  * interpretation of alpha isn't that simple:
    + http://projects.csail.mit.edu/church/w/images/thumb/7/73/Dirichlet.png/400px-Dirichlet.png
    + https://stats.stackexchange.com/questions/244917/what-exactly-is-the-alpha-in-the-dirichlet-distribution/244946#244946
    + https://www.hakkalabs.co/articles/the-dirichlet-distribution
    + http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/


# Accuracy

## Confusion Matrix
```{r results='asis'}
pp <- llply(s, crossTabProbs)
for(i in names(pp)) {
  print(kable_styling(kable(pp[[i]], format='html', caption = i), full_width = FALSE))
}
```


## Brier Scores
https://en.wikipedia.org/wiki/Brier_score

Brier scores [@Harrell2001] quantify agreement between observed classes and predicted probabilities:
$$ B = \frac{1}{n} \sum_{i=1}^{n}{ ( p_{i} - y_{i} )^{2}  }  $$
where $B$ is an index of agreement between predicted probabilities, $\mathbf{p}$, and class labels, $\mathbf{y}$. Larger values suggest less agreement between probabilities and observed class labels.

### TODO

  1. What about a weighted version of this score, based on a re-statement of the distance matrix?
  2. Add general purpose `brierScore()` function to aqp, adapt from `get.ml.hz()`.


## Tau and Weighted Tau (class-similarity)

[@Rossiter2017] implemented in `aqp::tauw()`. This paper contains some discussion on a weighted version of Shannon Entropy using the subset of similarities between predicted classes and the *actual* class.

```{r eval=FALSE}
## from tauw manual page.

# example confusion matrix
# rows: allocation (user's counts)
# columns: reference (producer's counts)
crossclass <- matrix(data=c(2,1,0,5,0,0,
                            1,74,2,1,3,6,
                            0,5,8,6,1,3,
                            6,1,3,91,0,0,
                            0,4,0,0,0,4,
                            0,6,2,2,4,38),
                     nrow=6, byrow=TRUE)
row.names(crossclass) <- c("OP", "SA", "UA", "UC", "AV", "AC")
colnames(crossclass) <- row.names(crossclass)

# build the weights matrix
# how much credit for a mis-allocation
weights <- matrix(data=c(1.00,0.05,0.05,0.15,0.05,0.15,
                         0.05,1.00,0.05,0.05,0.05,0.35,
                         0.05,0.05,1.00,0.20,0.15,0.15,
                         0.15,0.05,0.25,1.00,0.10,0.25,
                         0.05,0.10,0.15,0.10,1.00,0.15,
                         0.20,0.30,0.10,0.25,0.20,1.00),
                  nrow=6, byrow=TRUE)

# unweighted accuracy
summaryTauW(nnaive <- tauW(crossclass))

# unweighted tau with equal priors, equivalent to Foody (1992) modified Kappa
tauW(crossclass)$tau

# unweighted tau with user's = producer's marginals, equivalent to original kappa
(priors <-  apply(crossclass, 2, sum)/sum(crossclass))
tauW(crossclass, P=priors)$tau

# weighted accuracy; tau with equal priors
summaryTauW(weighted <- tauW(crossclass, W=weights))

# weighted accuracy; tau with user's = producer's marginals
summaryTauW(tauW(crossclass, W=weights, P=priors))

```


## Comparison
```{r}
pp <- ldply(s, performance)
names(pp)[1] <- 'example'

kable_styling(kable(pp, row.names = FALSE, digits = 2, format='html'), full_width = FALSE)
```


# Uncertainty

## Shannon Entropy

A normalized version [@Kempen2009] of Shannon entropy can be calculated:

$$ H = -\sum_{i=1}^{n}{p_{i} * log_{n}(p_{i})}  $$
where $H$ is an index of uncertainty associated with predicted probabilities, $\mathbf{p}$, of encountering classes $i$ through $n$. Values range from 0 (maximum information, minimum entropy) to 1 (minimum information, maximum entropy).

Examples from the simulated data:

```{r}
ex <- ldply(s, extractExample, n=1)
names(ex)[1] <- 'example'
ex$actual <- NULL
ex$CI <- NULL

column_spec(kable_styling(kable(ex, row.names = FALSE, digits = 2, format='html'), full_width = FALSE), 7, bold=TRUE)
```


### Weighted Shannon Entropy

A simple extension (weighted entropy score) of the standard version is described in [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4029130/):

$$ \mathit{WES} = -\sum_{i=1}^{n}{w_{i} * p_{i} * log_{n}(p_{i})}  $$
where $w_{i}$ is the weight associated with $p_{i}$. This isn't quite applicable as the weights (we are interested in) are based on pair-wise distances between classes. More promising ideas:

   * [mutual information](https://en.wikipedia.org/wiki/Mutual_information)
   * [pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) 
   * [zebu package vignette](http://cdn.rawgit.com/oliviermfmartin/zebu/master/inst/doc/zebu.html)
   * [joint entropy](https://en.wikipedia.org/wiki/Joint_entropy)

Pairwise mutual information seems like a convenient extension of Shannon H that would easily accommodate pairwise distances.

## Confusion Index

Following [@Burrough1997]:

$$ \mathit{CI} = [1 - (\mu_{max} - \mu_{max-1})] $$
where $\mathit{CI}$ is an index of confusion between the first most likely, $\mu_{max}$, and second most likely, $\mu_{max-1}$, class probabilities.

Examples from the simulated data:

```{r}
ex <- ldply(s, extractExample, n=1)
names(ex)[1] <- 'example'
ex$actual <- NULL
ex$Shannon.H <- NULL

column_spec(kable_styling(kable(ex, row.names = FALSE, digits = 2, format='html'), full_width = FALSE), 7, bold=TRUE)
```


## Shannon entropy vs. confusion index
Consider a probabilistic soil class model, that generates predictions for 5 possible classes. Suites of predicted probabilities fall into 3 general cases:

```{r}
ex <- ldply(s, extractExample, n=1)
names(ex)[1] <- 'example'
ex$actual <- NULL

kable_styling(kable(ex, row.names = FALSE, digits = 2, format='html'), full_width = FALSE)
```


```{r, fig.width=10, fig.height=4, fig.cap='Probability distributions of model predictions, 1000 simulations.'}
# examples of three cases
print(p.1)
```


```{r, fig.width=10, fig.height=4, fig.cap='Distribution of H and CI values computed from simulated predictions.'}
# uncertainty metrics
print(p.2)
```

```{r fig.width=10, fig.height=5, fig.cap='Relationship between H and CI values. Annotations are medians (IQR) and Spearman rank correlation.'}
print(p.3)

# kable_styling(kable(stats.cor, row.names = FALSE, digits = 2, format='html'), full_width = FALSE)
```



# Real Examples: Sequoia Kings Canyon (CA792)

Confusion Index
![](seki-CI.jpg)

Shannon H
![](seki-H.jpg)


Still working on this.
```{r}
# load cached data
load('SEKI-example-data.Rda')


#### the following required because of prior scaling of Pr to {0,100}
#### and randomForest generates 0sw

## 0-inflation only affects marginal Pr distribution investigation
## looking at the entire distribution is strongly influenced by 0-inflation

# rescale to probabilities
x <- x / 100.0
# add a very small fudge factor to remove 0
x[which(x==0)] <- 1e-15

####
####


class.labels <- toupper(letters[1:ncol(x)])

dimnames(x)[[2]] <- class.labels
d <- as.data.frame(x)
d$id <- rownames(d)

# uncertainty
H <- apply(x, 1, shannon.H, b=2)
H.norm <- apply(x, 1, shannon.H, b=length(class.labels))
CI <- apply(x, 1, confusion.index)

# prep for plotting
m <- melt(d, id.vars = c('id'), measure.vars = class.labels)
g <- make.groups(H, CI)
```

```{r fig.width=7, fig.height=6}
hist(x[x > 0.001], main='Predicted Probabilities, All Classes')

hist(apply(x, 1, max), main='Max Probability per Pixel')


plot(apply(x, 1, median), H, col=rgb(0,0,0,alpha = 0.125), pch=16, xlab='Median Probability per Pixel', ylab='Shannon H', las=1)
plot(apply(x, 1, max), H, col=rgb(0,0,0,alpha = 0.125), pch=16, xlab='Max Probability per Pixel', ylab='Shannon H', las=1)

plot(apply(x, 1, median), CI, col=rgb(0,0,0,alpha = 0.125), pch=16, xlab='Median Probability per Pixel', ylab='CI', las=1)
plot(apply(x, 1, max), CI, col=rgb(0,0,0,alpha = 0.125), pch=16, xlab='Median Probability per Pixel', ylab='CI', las=1)


bwplot(value ~ variable, data=m, subset= value > 0.001, xlab='Class', ylab='Predicted Probabilities > 0.001')


cols <- brewer.pal(9, 'Set1')
tps <- list(superpose.line=list(col=cols, lwd=1, alpha=0.85))

densityplot( ~ value , groups=variable, data=m, subset= value > 0.001,
                    pch=NA, xlim=c(-0.1, 1.1), scales=list(alternating=3, x=list(tick.number=5)), xlab='Class Probability',
                    strip=strip.custom(bg=grey(0.85)), auto.key=list(columns=4, lines=TRUE, points=FALSE),
                    par.settings=tps, panel=function(...) {
                      gs <- seq(0,1, by=0.1)
                      panel.abline(v=gs, lty=3, col='grey')
                      panel.densityplot(...)
                    })



# uncertainty metrics
cols <- wes_palette("Zissou")[c(1,5)]
tps <- list(superpose.line=list(col=cols, lwd=2, alpha=0.85))

densityplot( ~ data, groups=which, data=g, as.table=TRUE, pch=NA, auto.key=list(columns=2, lines=TRUE, points=FALSE), strip=strip.custom(bg=grey(0.85)), scales=list(alternating=3, y=list(rot=0), x=list(tick.number=5)), xlab='', par.settings=tps, panel=function(...) {
  panel.grid(-1, -1, lty=3, col='grey')
  panel.densityplot(...)
})


xyplot(H ~ CI, asp=1, scales=list(alternating=1), xlab='Confusion Index', ylab='Shannon H',
       strip=strip.custom(bg=grey(0.85)), par.settings=list(plot.symbol=list(col='royalblue', pch=16, cex=0.85, alpha=0.25)), 
       panel=function(x, y, subscripts=subscripts, ...) {
         panel.grid(-1, -1, lty=3, col='grey')
         med.H <- round(median(y), 2)
         med.CI <- round(median(x), 2)
         iqr.H <- round(IQR(y), 2)
         iqr.CI <- round(IQR(x), 2)
         cor.xy <- round(cor(x, y, method = 'spearman'), 2)
         ann <- paste0('H: ', med.H, ' (', iqr.H, ')\n', 
                       'CI: ', med.CI, ' (', iqr.CI, ')\n',
                       'cor: ', cor.xy)
         panel.xyplot(x, y, subscripts=subscripts, ...)
         panel.abline(0, 1, lty=2)
         grid.text(ann, x = unit(0.05, 'npc'), unit(0.95, 'npc'), just = c('left', 'top'), gp = gpar(cex=0.75, font=2))
       })

```



```{echo=FALSE}
                     crags/talus basins/cirques glacial v. floors glacial v. walls mtn. slopes, low RO mtn. slopes, high RO plateaus riparian class.error
crags/talus                  151             22                 0                5                   0                    9       13        0       0.245
basins/cirques                31             96                 5                9                   3                   17        9       30       0.520
glacial v. floors              0             12               156               10                   3                    2        3       14       0.220
glacial v. walls               7             12                18              121                  11                   28        0        3       0.395
mtn. slopes, low RO            0              3                 6               12                 139                   23       12        5       0.305
mtn. slopes, high RO          16             18                 5               40                  42                   67        9        3       0.665
plateaus                      14              4                 2                3                   6                    8      145       18       0.275
riparian                       0             22                 9                4                   4                    2       11      148       0.260
```


# Resources

 * http://biostat.mc.vanderbilt.edu/wiki/Main/RmS

# References
fake text here








