---
title: "Loafercreek - offcuts"
author: "Andrew Brown"
date: "February 6, 2019"
output: html_document
---


***
#### Tip: Using logical vectors to index SPCs

Logical vectors are handy as indexes of `SoilProfileCollections` and various other R objects. A _logical expression_ is a piece of code that results in a _logical vector_.

In particular, _logical vectors_ are nice since you can return `TRUE` or `FALSE` after evaluating an _expression_ on __EACH PROFILE__ in a SPC. That is, you have a vector of equal length to the number of sites in the SPC. This is an important "sanity check" for your analyses.

_When using logical vectors as index variables it is less likely one of your profiles will accidentally "fall through the cracks" during analysis. You also are less likely to introduce unintended "offsets" causing the "wrong" profiles to be selected when you index. Also, if you try to use a logical vector containing `NA` to index an SPC, you will (rightly) get an error._

You can also index using just the "positions" you want (e.g. `spc[c(2, 3, 10), ]` to get the second, third and tenth profile in an SPC). 

You can convert a logical vector to a _vector of _positions where the logical vector is `TRUE`_ using the function `which()`.

For instance:

```{r}
a <- c(TRUE, FALSE, FALSE, TRUE)

a # compare this

which(a) # to this
```

You can _invert_ logicals (take the opposite; `TRUE` to `FALSE` & `FALSE` to `TRUE`) using the exclamation mark `!` (_NOT_) operator. 

_logical_ comparison (_equals_: `==`; _does not equal_: `!=`) is done between the values in the _same index position_ of two vectors being compared. T

hat means that when making _logical expressions_, vectors should either be the _same_ length, or one of _n_-length and the other of length 1.

You can also evaluate a logical expression using `&` (_AND_) and `|` (_OR_) operators. 

 * In order for an expression to return `TRUE`, _AND_ requires __both__ sides of the expression to be `TRUE`.
 
 * Whereas _OR_ returns `TRUE` with __either__ or __both__ sides of the expression being `TRUE`.

<details>
<summary>
_Test your understanding [CLICK HERE]:_ 
</summary>

Estimate the result of the following logical expressions; check answers in your R console.

```{r eval=FALSE}
a <- c(TRUE, FALSE, TRUE)
b <- c(FALSE, TRUE, FALSE)

a & b
a | b

a == b
a != b
a == !b

sum(a)
sum(b)

all(a)
any(b)

is.na(a)
is.na(a & NA)
is.na(a | NA)

(TRUE & NA) != (FALSE & NA)
(FALSE | NA) | (TRUE | NA)
(TRUE | NA) & (FALSE & NA)
```
</details>

***

#### using a datafram when you meant to use an spc

```
# there is no method for coercing a data.frame to SPDF (you need spatial info)
## loafercreek.spdf <- as(site(loafercreek), 'SpatialPointsDataFrame')
```
### Patterns in soil color?

Many series criteria historically have been based on attributes related to depth to "redness", or "redness" in general. We want to do a pedon summary that reflects this. 

We will split the `loafercreek` dataset into decent size "groups" that we can summarize for this demo based on dry hue `loafercreek$d_hue`.

To begin we redefine the default `name` in the `estimateSoilDepth()` function to reference the dry hue (`d_hue`). The character string supplied for `p` is a regex pattern. 

The pattern `p` matches _character_ (strings) that __(start with '5YR') OR (start with '2.5YR')__. [For convenience I will call this "redness" though I acknowledge this is a simplistic way of dividing based on color.]

We use `d_hue` because it is an unusual attribute to plot on its own, and it has data type of _character_. Typically, `estimateSoilDepth()` is designed to operate on the horizon designation field `hzname`, which is also a _character_. This is an example of how you can alter common workflows for new analyses.
The depth of the _first_ horizon with dry hue matching the pattern is returned for each profile.

```{r}
loafercreek$depth.to.5YR <- profileApply(loafercreek, FUN = estimateSoilDepth, 
                                         name = 'd_hue', p = '^5YR|^2\\.5YR')

# kernel density estimate
dens <- density(loafercreek$depth.to.5YR, na.rm = TRUE, from = 0, to = 100)

# plot the result and modify title, y-axis text orientation
plot(dens, main = "Distribution of depth to 5YR or 2.5YR dry hue in Loafercreek", las=1)
```

Interesting. Typically, if you were summarizing a homogenous group, you would expect a unimodal distribution, even if not perfectly _normal_. This looks a bit bimodal... there might be two clusters here? 

Wait a minute.

```{r}
loafercreek$depth.to.5YR
```

There are values of `200` and _over_ From `estimateSoilDepth()`.

Turns out, setting the result to an arbitrarily large number `200` is default behavior for profiles that never match the pattern. Some are greater than `200` and if you check, they are the bottom depths from the last (bedrock) layer in those pedons. 

Before we go any further, we will set `depth.to.5YR` to `Inf` (infinity) for any depth that is greater than the bedrock depth that we estimated earlier, using the default pattern (for Cr, R, Cd). It actually has no effect on the above density plot, since we set the probability estimates to cut off at 100 cm using `density(..., to = 100)`, and `200 > 100` and `Inf > 100`.

```{r}
loafercreek$depth.to.5YR[loafercreek$depth.to.5YR >= loafercreek$bedrckdepth] <- Inf
```

`Inf` is a good flag for this situation, because we didnt find the color we were looking for at any depth, so to have any finite _positive_ depth is somewhat misleading, and negative depths will get caught up in any less-than (`<` `<=`) operations. We can check for infinite values later if we need to with `is.finite()`.

Note that `density()` plots numbers of observations are misleading, because `Inf` isn't omitted like an `NA`, but doesn't contribute like real data.

We will split `loafercreek` into two separate SPCs based on `depth.to.5YR` and make profile plots for each. 

***

##### __Exercise__ 
_Consider an alternate definition of "redness" using dry color: 6 chroma or higher, allowing 7.5YR hues, omitting 5YR -/4 chroma or less, and allowing all 2.5YR hues? What attributes in `loafercreek` would you need to incorporate? [you don't have to make a function to do it all right now]_

_Inspecting the data, how would you estimate using this definition would change the results (depths to "redness" estimated)?_

_What other definitions of "redness" can you come up with?_

***

We are going to make a cut at 40 cm provisionally, based on the two "peaks" in the density plot. Our hypothesis was that soils shallow to red hues would have a different morphology (we will look at clay content). This break coincides with the data and our working hypothesis. We could have just said 50 cm from the outset without looking to see if that was a relevant break for our data.

Subset `sub1` is created using _aqp_ function `subsetProfiles()`. It consists of soils where `depth.to.5YR` is less than or equal to 40 cm. `s` is an expression (character string) to evaluate for a variable in the `loafercreek@site` table. Another argument, `h`, not specified in this case, does the same thing but matches a horizon-level attribute in `loafercreek@horizons`.

# High-level function demos

Some functions are 'vectorized' or otherwise are prepared to handle iteration over elements in an object on their own (no `profileApply()` needed). 

Or they chain together the results of low-level functions to produce a more complicated, but useful [?], result. 

You might benefit from designing high-level functions to automate common workflows you do in e.g Excel, or do manually in R.

## `getSoilDepthClass()`

An example from the _aqp_ package is `getSoilDepthClass()` which internally makes use of `profileApply()` and `estimateSoilDepth()`.

```{r}
sdc <- getSoilDepthClass(loafercreek)
```

`sdc` is a _data.frame_ (1 row per site) with several different representations of soil depth:

 * numeric depth (e.g. top depth of restriction)
 * presence/absence (TRUE/FALSE) of contact by class
 * categorical factor of depth class

Look at the first few records to see result _data.frame_ format.
```{r}
head(sdc, 3)
```

You can define your own depth class names and cutoff depths using the `depth.classes` argument.

Let's look at the breakdown of depth class in `loafercreek`. The variable `depth.class` in _data.frame_ `sdc` is a `factor`, so `summary()` counts the number of observations in each factor level (depth class). 

We calculate the number of pedon observations (rows in `loafercreek@site` `n.obs`) and combine that total with the summary output to calculate proportions.

```{r}
n.obs <- length(loafercreek)
names(n.obs) <- "total"
loafercreek.depth.summary <- summary(sdc$depth.class)
```

__List with number of observations per class, plus total in own column__
```{r}
c(loafercreek.depth.summary, n.obs)
```

__List with percentages by class__
```{r}
round(c(loafercreek.depth.summary / n.obs) * 100, digits = 1)
```

The density plot we made with `estimateSoilDepth()` and the categorical summary of `getSoilDepthClass()` output both show as expected that the soils are mostly moderately deep (50 - 100cm). This is consistent with the Loafercreek series concept.

## `aggregateColor()`

We can use the `aggregateColor()` function to summarize the colors in a SPC by horizon designation. It returns a two element list containing _scaled_ and _aggregate_ data. 

_Scaled_ data is a _list_ of colors and their weights (proportion or number of horizons with a particular designation/group label). 

When you specify the argument `k` you get _aggregate_ color data. Aggregate data is a _data.frame_ of "best representative" colors. Munsell colors are converted to LAB color space (a different _numeric_ representation of color). 

In LAB space, mathematical operations using color data are more feasible than they are using Munsell notation.

`aggregateColorPlot()` is a special plotting function designed to use the results of `aggregateColor()`. We set the option to display the number of horizons contributing dry color data to each group.

But first, we will create a _new_ grouping variable for horizons. We will make it from the _generalized_ (correlated) master designation which is loaded in `loafercreek$genhz` (from Pedon Horizon Component Layer ID in NASIS).

Instead of `genhz`, we could do this analysis from `hzname`. However, the field horizon designation (`hzname`) has more variation (lithologic discontinuities, different soil scientists horizonation styles). That extra variation would require us to devlop more complicated regular expression patterns to match horizon labels to groups. 

We want very general groups here so `genhz` (component layer ID in NASIS) will do just fine.

For this demo I wanted to keep the horizon grouping very simple. Even simpler than `genhz` itself. Mainly, we are seeking less subdivision of the argillic horizon and more consistent use of upper and lower transitional horizon labels.

We make some general rules for what is an A horizon, what is transitional, what is argillic, what is lower gradational and bedrock and save the groupings in a new variable `extragenhz`.

```{r}
#copy generalized horizon labels (NASIS component layer ID) to new variable
loafercreek$extragenhz <- loafercreek$genhz

# NOTE that the order of the following 5 statements is **important** 
# (they can overwrite each other)

# if it contains A it goes in the A group
loafercreek$extragenhz[grepl(loafercreek$genhz, pattern='A')] <- 'A'

# if starts with B, and has a t, it goes in Bt group
loafercreek$extragenhz[grepl(loafercreek$genhz, pattern='^B.*t.*')] <- 'Bt'

# if it starts with BC (with or without t) its in the 
# "lower gradational to bedrock group" BCt
loafercreek$extragenhz[grepl(loafercreek$genhz, pattern='^BC')] <- 'BCt'

# any start with C? [no]
## loafercreek$extragenhz[grepl(loafercreek$genhz, pattern='^C')] <- 'C'

# bedrock colors - usually weird and rarely populated
loafercreek$extragenhz[grepl(loafercreek$genhz, pattern='Cr|R|Cd')] <- 'Cr'

loafercreek$extragenhz <-  factor(loafercreek$extragenhz,
                                  levels = c('A','Bt','BCt','Cr'))
```

Show all dry colors in the `loafercreek` SPC  -- grouped by our extra-generalized horizons `loafercreek$extragenhz`.

```{r fig.width=10}
library(sharpshootR)
# fix margins
par(mar=c(4.5,2,0,0))

aggregateColorPlot(aggregateColor(groups='extragenhz', 
                                  col ='dry_soil_color', 
                                  loafercreek), 
                   label.cex = 1,
                   print.n.hz = TRUE)
```

Use the code below to show three "best" Munsell chips for each horizon label. 

The "best" chips are determined after converting Munsell to LAB color coordinates, clustering (i.e. grouping similar colors together) and converting cluster medoids back to Munsell notation. 

LAB colors are clustered into a user-specified number (`k`) of groups using Partitioning Around Medoids. 

For each group, the medoid LAB color is back-transformed to nearest Munsell chip and returned. Then the weight of all of the observations in the cluster is assigned to that chip. 

The math behind the clustering is beyond the scope of this demo, but the result when specifying `k` in a call to `aggregateColor()` is that you get back `k` different colors that represent the data best. This has the effect of aggregating similar colors into a single chip, which simplifies the output. 

For a given dataset, larger the value of `k` the more similar the medoid colors will be to one another. That is, as you increase `k` each color cluster will partition less variation. If the input data are _very_ similar, different cluster medoids theoretically could back-transform to the same Munsell chip. This is because LAB allows for much greater "precision" than whole-chip Munsell colors, and if you force it to make more clusters than are needed, those clusters could come back as the "same".

```{r fig.width=10}
lt40.to.red <- loafercreek[loafercreek$redness.class == 'LT40', ]

# fix margins
par(mar=c(4.5,2,0,0))

aggregateColorPlot(aggregateColor(groups='extragenhz', 
                                  col='dry_soil_color', 
                                  lt40.to.red, 
                                  k = 3),  
                   label.cex = 1,
                   print.n.hz = TRUE)
```

```{r fig.width=10}
gt40.to.red <- loafercreek[loafercreek$redness.class == 'GT40', ]

# fix margins
par(mar=c(4.5,2,0,0))

aggregateColorPlot(aggregateColor(groups='extragenhz', 
                                  col='dry_soil_color', 
                                  gt40.to.red, 
                                  k = 3),  
                   label.cex = 1,
                   print.n.hz = TRUE)
```

In addition to the 5YR hue, the `LT40` group has 6 chroma represented at somewhat higher proportions. The chips with same value/chroma are pretty s Thus, there is not a huge amount of difference between these groups when their colors are looked at in aggregate. The raw color plots show more detail (without the `PAM`-based mixing and `k = 3` argument to `aggregateColor()`), but more or less tell the same story.

However, even though we didn't find a difference more than the result of our arbitrary split, applying a calculation "Depth to 5YR hue", we had the opportunity to explore the natural variability within a group of related soils as well as the gaps in our data.

Lets investigate how these "redness" groups, however intermingled they may be, to see if they have different clay contents.

## `slab()` and `slice()`

The _aqp_ `slab()` function takes constant-thickness slices from multiple pedons and aggregates them using a function (`slab.fun`). This has the effect of "smoothing" the between-pedon variation to give depth-wise quantile estimates for properties within a SPC.

Multiple continuous variables OR a single categorical (factor) variable (in this case `redness.class`) can be aggregated within a call to `slab()`. Slab returns the results of running arbitrary functions on groups of slices. 

Below we expand the default `slab()` function `slab.fun=aqp:::.slab.fun.numeric.default` to include the mean with a user-defined function `slab.default.plus.mean()`.

```{r}
slab.default.plus.mean <- function(value) {
  # make a named numeric vector containing the mean of value
  the.mean <- mean(value, na.rm=TRUE)
  names(the.mean) <- "avg"
  
  # combine mean with the .slab.fun.numeric.default 
  # used in the slab() function definition
  return(c(aqp:::.slab.fun.numeric.default(value), the.mean))
}

# slab loafercreek SPC using user-defined slab.fun; 
# summarize clay content by redness class (in 5cm depth "slabs")
loaf.slab <- slab(loafercreek, redness.class ~ clay, slab.fun=slab.default.plus.mean, slab.structure=5)
```

When you use `slab`, profiles are first `slice()`-d into constant-depth (_thin,_ _relative to horizons_) sections.

We used `slice()` earlier in `depth.weighted.average()` to calculate a depth-weighted average. `slice()` returns a SPC (useful for plotting) or just the sliced data (`just.the.data=TRUE`) in a data frame. 

The simplest `slab` (the least aggregated) uses the default 1 cm thick structure. By default the `slab.structure` is set to bands of 1 cm vertical thickness (`slab.structure=1`). which is the same thickness as `slice()`. 

```{r}
# inspect the first few records in the slab we just made
head(loaf.slab)
```

In this demo, we set `slab` structure (constant thickness) to 5 cm. This was to increase the amount of depth-wise aggregation _and_ the amount of data contributing (`loaf.slab$contributing_fraction`) to each 5 cm slab. 

Using the above setup, five 1 cm thick slices will contribute to each slab _from each profile_. The slabs will be 5 cm increments starting from the minimum top depth to the maximum bottom depth in the entire SPC. Data from _all_ profiles will contribute to each slab (if non-NA).

Note: `slab()` and `slice()` are much faster and require less memory if input data are either _numeric_ or _character_.

 * Watch out for future specific demos on using these functions.

___Example: ___ _Loafercreek slab-wise clay lattice plot demo_

#### Data contributing to a slab

There are good and _not so good_ reasons why the contributing fraction (`loaf.slab$contributing_fraction`; % of profiles) for a particular slab isn't 100%. 

In soils that end at a bedrock contact, like _Loafercreek_, or with descriptions to varying bottom depths, the contributing fraction drops as you exceed the bottom depth of the "shallowest" pedons in the set. 

It is OK that the contributing fraction drops off when non-soil layers (i.e. bedrock) are summarized. It is _less OK_ when the contributing fraction drops because the description stopped before the end of the series control section or because data are missing for a soil horizon when they could have been recorded. __Always describe the entire series control section.__ 

The decrease in contributing fraction is partly due to missing data for clay content (O horizons, bedrock, and true missing data). 

Quantiles estimated from very small proportions of data tend to be unstable relative to the rest of the profile. 

Therefore, we take a subset (the depth range) of `loaf.slab` where the `contributing_fraction` of pedons is greater than 25% (`0.25`). 

```{r}
# excluding slabs with contributing fraction < 25% for quantile stability; 
# this cutoff is not a hard rule and depend on the data.
loaf.slab.sub <- loaf.slab[loaf.slab$contributing_fraction >= 0.25,]

# check out the slab contents (variable, group, summary stats, hz depths, contributing fraction)
# note the data.frame is in long format not wide format
head(loaf.slab.sub)
```

#### Visualizing slab-wise soil profile data

Load the `lattice` package, and create an XY plot of the slab-wise median and interquartile range as a function of depth.

Use the same grouping variable you supplied to `slab()` (`redness.class`) to create separate plots for each group.

```{r}
library(lattice)

# define plotting style
tps <- list(superpose.line=list(col=c('RoyalBlue'), lwd=2))

# lattice graphics make it (a little) simple to plot grouped data
xyplot(top ~ p.q50 | redness.class, data = loaf.slab.sub, 
       main = 'Loafercreek - depth distribution of clay by \"redness class\"',
       ylab = 'Depth, cm',
       xlab = 'Median bounded by 5th and 95th percentiles',
       lower = loaf.slab.sub$p.q5, upper = loaf.slab.sub$p.q95, 
       xlim = c(0, 60),
       ylim = c(100,-5),
       panel = panel.depth_function, 
       prepanel = prepanel.depth_function,
       cf = loaf.slab.sub$contributing_fraction,
       sync.colors=TRUE, alpha=0.25,
       par.settings=tps,
       strip=strip.custom(bg=grey(0.85)),
       layout = c(3,1), scales = list(x = list(alternating = 1))
  )
```

__Calculate the maximum clay content from the 5cm slab-wise medians__
```{r}
df <- aggregate(loaf.slab.sub$p.q50, by = list(loaf.slab.sub$redness.class), FUN = max, na.rm = TRUE)
names(df) <- c("redness.class","maxclay.q50")

# print the results
df
```

A `r round(df[1,2] - df[2,2], 1)`% clay difference in the estimated maxima using the slab-wise median (50th percentile). The clay content maxima of the _slab-wise median_ values are smaller than we calculate from the raw maximum clay content data (see below code). 

That might be expected of a statistical summary of several pedons across depth, in that the estimate you get is more smoothed out. In this case, comparing the slab wise median to the absolute maximum clay might not be totally fair. It might be more appropriate to compare to a more extreme quantile [which?] for `maxclay` difference.

More interesting than _peak median clay content_ is the _differences in 5th to 95th percentile intervals across depth_ in our different groups. 

Remember 10% of the data fall outside of the 5th-95th percentile range for each 5 cm thick depth slab. The groups have similar numbers of observations, and overall relatively similar morphology, but the LT40 group appears to have more variation within it, especially in the higher clay content direction. This not being reflected in the median suggests it may be only a handful of observations influencing the pattern.

#### Quantile-based summaries of unslabbed results

We will use quantiles to summarize the distribution of `maxclay` for our three different redness groups and compare these values to the smoothed slab-wise quantiles derived above. Some extra effort is used to prepare a more legible table.

```{r}
# define some quantiles of interest
probs <- c(0,0.05,0.25,0.5,0.75,0.95,1)

# split the max clay values by redness class into a list
s <- split(loafercreek$maxclay, f = loafercreek$redness.class)
# compute desired quantiles within each list element
a <- lapply(s, quantile, na.rm = TRUE, probs = probs)
# combine into a data.frame
a <- as.data.frame(do.call('rbind', a))

# fix column names
names(a) <- paste('q', probs, sep='')

# note that group labels are stored in row.names
# pretty-print the table, automatically converted to HTML in an Rmarkdown document
knitr::kable(a)
```


The _median_ (`q0.5`) we calculate above for each group is not considering horizon thickness. These medians show larger differences between groups than we found in the slab-wise analysis. This make sense, because all obseravations have equal weight in the above aggregate call, whereas slab effectively weights them based on their depths relative to the user defined slab structure.

Think of it this way: when calculating max clay for a profile... a horizon with 60% clay that is 3 cm thick counts just as much as a horizon that is 50cm thick. 

However, a 3 cm thick contribution to a 5cm thick slab will only be about half of the contributing data from that pedon to that slab, or _maybe two_ slabs depending on the depths of the horizon. Whereas a 50cm thick horizon will occupy most of _at least 10_ 5 cm thick slabs. 

Despite apparent differences in the median, the ranges of the groups are quite similar overall, in terms of textural classes.

Most of these pedons, but not all, fall within the fine-loamy particle size family range of 18-35% clay in the particle size control section. All methods we used to look at clay maximum suggest slight differences in group composition, but not so much meaningful differences given the precision of typical field clay content estimates.

On one hand, this is expected since we started with a relatively constrained group of pedons (_fine-loamy, mod. deep, with argillic, more or less from similar areas_). 

On the other hand, the pedons spill outside of that neat parenthetical box.

There appears to be more variation within the `LT40` group (evidenced by the shaded area in the `lattice` plot). 

The variation is probably not pronounced or predictable enough to affect use and management in these areas (dominantly rangeland). 

In practice, it might be worth investigating the pedons in that group further to see if there are commonalities in where they occur on the landscape, if they have other unique soil forming factors and/or if they have been adequately sampled.

 * Sequel? spatial comparison of clay content and color groups in _Loafercreek_

</cliffhanger>

***

# Challenge Exercise

Data are available in _soilDB_ `gopheridge`! 

_Do a_ `profileApply()` _-based analysis of_ __rock fragments__ - _comparing Loafercreek and its skeletal sibling Gopheridge._

Some code to get you started.

```{r , eval = FALSE}
library(soilDB)
data("loafercreek") # WARNING: resets any local R environment changes YOU MADE to `loafercreek`
data("gopheridge")

loafergopher <- union(list(loafercreek, gopheridge)) 

# convert all taxon names to upper case
loafergopher$taxonname <- toupper(loafergopher$taxonname)

groupedProfilePlot(loafergopher[c(1,8,123,140), ], 
                   print.id = FALSE, axis.line.offset = -1,
                   color = 'total_frags_pct', 
                   groups = "taxonname", 
                   max.depth=150,
                   group.name.offset = -10,
                   group.name.cex = 0.5)
```

***

# Future directions

Recently code was added to _aqp_ to calculate the upper and lower boundary of the particle size control section using `estimatePSCS(pedon)`. This function returns two numeric values corresponding to the upper and lower bounds of the PSCS. These bounds can be used for selecting horizon data, calculating weighted averages over that interval, etc.

`estimatePSCS()` is a high-level function (makes use of multiple lower level functions) to apply the PSCS key from _Keys to Soil Taxonomy_. It requires "machinery" for the identification of argillic horizons, O horizons, plow layers etc. It also needs to be able to deal with regional differences in description styles, data entry guidelines and ambiguities in data population relative to taxonomic criteria.

There is still no handling of some "unusual" soils (well, unusual in that they would be a huge amount of effort to cover a minute fraction of observations) e.g. grossarenic subgroups, strongly contrasting etc. Some of these can be added if there is a need or interest. Others might be more effort than it is worth and will in all likelihood have to rely on some sort of an external flag (like a record in the taxonomic history table). 

If someone has a specific need for additional functionality, and has a dataset to use test with, the author would be willing to explore implementation of any of these more complex criteria.

This area will have links to more pedon summary demos when they become available.

Here is a link to describe how these functions fit into the aqp/sharpshootR ecosystem.
  